{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cabecera_rl.png](./img/cabecera_rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Datos de partida**](#**Datos-de-partida**)  \n",
    "\n",
    "* [**Código a completar**](#**Código-a-completar**)  \n",
    "\n"
   ]
  },
  {
   "attachments": {
    "ejercicio.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ejercicio.png](attachment:ejercicio.png) **EJERCICIO COMPLETAR CODIGO**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMARTCAB, mejorando una estrategia nada óptima\n",
    "\n",
    "El siguiente código mejora ligeramente las prestaciones de nuestro SmartCab sin algoritmo de aprendizaje por refuerzo. Para ello, añadimos una pequeña pero útil comprobación: no intentaremos hacer recogidas de pasajeros en los puntos que no son de recogida, ni intentaremos dejar al pasajero en puntos que no son el destino. \n",
    "\n",
    "Revisa y completa el código de forma que se ejecute sin errores y comprueba que la solución final tiene 0 penalties y además el número de epochs es sensiblemente menor a 2800.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**IMPORTANTE:** Si lo prefieres, descargarte el notebook (File -> Download as -> Notebook) y complétalo en local, pero en ese caso tendrás que instalarte (via pip o conda) las librerias necesarias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Datos de partida**  \n",
    "\n",
    "[al indice](#Contenidos)  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer el ejercicio necesitas conocer los valores de las observaciones. \n",
    "\n",
    "Si ejecutas:\n",
    "observations = list(env.decode(state)), donde state contiene el estado del entorno, observations es una lista de valores con las observaciones:  \n",
    "observations[0] = Fila de la posición del taxi (valor x)  \n",
    "observations[1] = Columna de la posición del taxi (valor y)  \n",
    "observations[2] = Estado del pasajero, (0-3, las posiciones R,G,B o Y, 4, está en el taxi)   \n",
    "observations[3] = Posición de destino (0-3, para las posciones R,G,B o Y)  \n",
    "\n",
    "Además las localizaciones de los puntos de recogida/destino son:  \n",
    "(0,0), posicion 0  \n",
    "(0,4), posicion 1  \n",
    "(4,0), posicion 2  \n",
    "(4,3), posicion 3  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Código a completar**  \n",
    "\n",
    "[al indice](#Contenidos)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número medio de acciones ejecutadas por episodio: 593.25\n",
      "Número medio de penalizaciones por episodio: 0.0\n",
      "Tiempo medio simulado por episodio: 26756.25\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env\n",
    "\n",
    "penalties, reward, epochs = 0, 0, 0 # Inicializa las variables que llevaran las métricas del \"entrenamiento\"\n",
    "simulated_time = 0\n",
    "\n",
    "done = False\n",
    "\n",
    "def es_destino_o_recogida(state_index):\n",
    "    # Obtiene las observaciones correspondientes al indice indicado en state_index\n",
    "    observations = list(env.decode(state_index))\n",
    "    \n",
    "    # Extraer la fila y la columna del taxi\n",
    "    taxi_row, taxi_col = observations[0], observations[1]\n",
    "    \n",
    "    # Extraer índice de ubicación del pasajero y índice de destino\n",
    "    passenger_index, destination_index = observations[2], observations[3]\n",
    "    \n",
    "    # Ubicaciones codificadas como (fila, columna) para los índices 0, 1, 2, y 3\n",
    "    locations = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
    "\n",
    "    # Verificar si la posición del taxi coincide con la ubicación de recogida o la ubicación de destino\n",
    "    es_el_punto_de_recogida = (taxi_row, taxi_col) == locations[passenger_index] if passenger_index < 4 else False\n",
    "    es_el_punto_de_destino = (taxi_row, taxi_col) == locations[destination_index] if passenger_index == 4 else False\n",
    "    \n",
    "    return 1 if es_el_punto_de_destino else 2 if es_el_punto_de_recogida else 0\n",
    "\n",
    "for episode in range(100): # Jugamos 100 episodios y haremos las medias de las metricas de cada uno de ellos\n",
    "    state, info = env.reset() # Reset the environment to start a new episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample() # Escoge una acción de forma aleatoria\n",
    "\n",
    "        # Comprueba si la acción es dejar y el pasajero no está en el taxi\n",
    "        if action == 5 and es_destino_o_recogida(state) != 1:\n",
    "            continue # Este comando hace que se salte todo lo demás y comience el bucle de nuevo, es decir no se actualizan contadores\n",
    "        \n",
    "        # Comprueba si la acción es recoger y la localización no es un punto de recogida o el pasajero no está en un punto de recogida\n",
    "        if action == 4 and es_destino_o_recogida(state) != 2:\n",
    "            continue # Este comando hace que se salte todo lo demás y comience el bucle de nuevo, es decir no se actualizan contadores\n",
    "        \n",
    "        if action < 4:\n",
    "            simulated_time += 45\n",
    "        else:\n",
    "            simulated_time += 75\n",
    "        \n",
    "        state, reward, done, truncated, info = env.step(action) # Ejecuta la acción\n",
    "        if reward == -10: # Si se equivoca en recogidas o dejadas, añade uno al contador de penalizaciones\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "print(\"Número medio de acciones ejecutadas por episodio: {}\".format(epochs/100))\n",
    "print(\"Número medio de penalizaciones por episodio: {}\".format(penalties/100))\n",
    "print(\"Tiempo medio simulado por episodio: {}\".format(simulated_time/100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
